{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d11c0-bad6-41b5-93fa-47f8d9574931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91fc38-1e3a-4151-8b4b-1f7a4681538d",
   "metadata": {},
   "source": [
    "Web scraping is the process of automatically extracting data from websites. It involves using software tools or scripts to navigate web pages, extract relevant information, and save it in a structured format, such as a spreadsheet or a database. Web scraping allows users to access and collect data that would otherwise be time-consuming or challenging to obtain manually.\n",
    "\n",
    "Web scraping is commonly used to retrieve the most updated data about properties, sale prices, monthly rental income, amenities, property agents, and other data points. Web scraped data also informs property value appraisals, rental yield estimates, and real estate market trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6135c7-3a85-4fad-b7c5-afaac2ec297d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8154d27-88a8-49bf-b4f5-c6d5251f5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddd730-9fe3-42ba-961d-a41d4943b095",
   "metadata": {},
   "source": [
    "\n",
    "Web scraping can be performed using various methods and techniques, depending on the complexity of the task and the structure of the websites being scraped. Here are some common methods used for web scraping:\n",
    "\n",
    "1) Using Web Scraping Libraries: There are several web scraping libraries available in different programming languages, such as Python's BeautifulSoup, Scrapy, or requests-html. These libraries provide built-in functions and tools to extract data from HTML or XML documents easily.\n",
    "\n",
    "2) XPath: XPath is a query language used to navigate through XML and HTML documents. It allows web scrapers to locate specific elements or nodes on a web page by defining their paths. XPath is often used in conjunction with libraries like lxml in Python.\n",
    "\n",
    "3) Regular Expressions (Regex): Regular expressions are powerful pattern-matching tools used to find specific patterns in text data. While not specific to web scraping, they can be employed to extract information from web pages, especially when the data has a well-defined pattern.\n",
    "\n",
    "4) APIs (Application Programming Interfaces): Some websites offer APIs that allow developers to access and extract data in a structured format. APIs are a more reliable and ethical way of obtaining data compared to traditional web scraping as they are specifically designed for data exchange.\n",
    "\n",
    "5) Headless Browsers: Headless browsers, like Puppeteer (for JavaScript) and Selenium (for various languages), simulate the behavior of a web browser without a visible user interface. This allows the scraper to interact with dynamic websites that rely heavily on JavaScript to load content.\n",
    "\n",
    "6) Scraping Frameworks: In addition to libraries, there are full-fledged scraping frameworks like Scrapy (Python) and Nutch (Java) that provide a comprehensive set of tools to manage the entire web scraping process, from crawling to data extraction.\n",
    "\n",
    "7) Web Scraping Services: Some companies offer web scraping services, allowing users to outsource their data extraction needs. These services handle the technical aspects of scraping, allowing users to focus on analyzing the data.\n",
    "\n",
    "8) Browser Extensions: Certain browser extensions, like Data Miner, allow users to extract data from websites visually by interacting with the page elements and exporting the results in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f09573-ba61-494b-82f7-5ebf00754fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d46b0f-6342-4c2a-b42f-973240772eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561d3b3-10c1-4fec-80e5-98eb1cf0d020",
   "metadata": {},
   "source": [
    "\n",
    "Beautiful Soup is a popular Python library used for web scraping tasks. It is designed to parse HTML and XML documents, allowing users to extract data from web pages easily. Beautiful Soup provides a simple and Pythonic way to navigate and search the parse tree of a webpage, making it a valuable tool for web scraping projects.\n",
    "\n",
    "\n",
    "Here are some key features of Beautiful Soup and reasons why it is widely used:\n",
    "\n",
    "1) HTML/XML Parsing: Beautiful Soup can parse HTML and XML documents, converting them into a parse tree. This parse tree represents the structure of the document, making it easy to navigate and search for specific elements.\n",
    "\n",
    "2) Easy to Use: Beautiful Soup's syntax is straightforward and easy to understand, even for beginners. It allows developers to extract data from HTML documents using Pythonic idioms, making the code readable and maintainable.\n",
    "\n",
    "3) Tag Navigation: With Beautiful Soup, you can navigate the parse tree using tags and attributes. It provides methods to access specific HTML elements and extract data from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763c04f-1af5-4350-9a5c-f157501f9c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155ca61-6b35-4885-b670-7b993e50617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a1fde-8ba7-4401-b934-3b491ccc442e",
   "metadata": {},
   "source": [
    "Flask is a popular web framework in Python that is commonly used to build web applications and APIs. In the context of a web scraping project, Flask can be used for various reasons:\n",
    "\n",
    "1) RESTful API: Flask allows you to create a RESTful API easily. When combined with web scraping, this API can serve as an interface to access the scraped data. Users can make HTTP requests to the Flask app's API endpoints and retrieve the scraped data in a structured format, such as JSON.\n",
    "\n",
    "2) Data Presentation: Flask enables you to create web pages or endpoints where the scraped data can be presented in a user-friendly way. This could involve rendering the data in HTML templates or building interactive visualizations to display the results.\n",
    "\n",
    "3) User Interaction: In some cases, web scraping projects may require user interaction, such as providing input parameters for the scraping process or allowing users to customize the data they want to retrieve. Flask makes it easy to handle user input and manage data flow accordingly.\n",
    "\n",
    "5) Asynchronous Scraping: Web scraping can sometimes be a time-consuming process, especially when dealing with a large number of pages or websites. Flask can be combined with asynchronous libraries like Celery to perform web scraping tasks in the background while still serving other API requests and web pages concurrently.\n",
    "\n",
    "6) Caching: To avoid making repetitive and unnecessary scraping requests, Flask can be used to implement caching mechanisms. Cached data can be served to users if it is still valid, reducing the load on the web scraping process and improving the application's performance.\n",
    "\n",
    "7) Deployment and Scalability: Flask applications are relatively easy to deploy and can run on various hosting platforms. This makes it convenient to deploy a web scraping project on servers and scale the application as needed.\n",
    "\n",
    "8) Error Handling and Logging: Flask offers robust error handling and logging capabilities, allowing you to log and monitor potential issues that may arise during the web scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f11e3a-7ca8-4716-8e6b-2cea1cc4db1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71454aff-a986-4627-b036-81cdd4bfa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347ae70-dc36-449c-98e7-f7a73b481ddb",
   "metadata": {},
   "source": [
    "AWS services used in this project are :\n",
    "\n",
    "1) Elastic Beanstalk:  It is used to create an environment t o deploy the application.\n",
    "\n",
    "2) CodePipeline  : Tt is used to fetch the source code from the github and pass it to the elastic Beanstalk.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
